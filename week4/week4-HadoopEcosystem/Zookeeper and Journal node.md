Zookeeper is a crucial piece of any Big Data deployment at enterprise scale. ZooKeeper is a centralized service for maintaining configuration information, naming, providing distributed synchronization and group services. All these services are used by distributed applications. According to the official [website](https://zookeeper.apache.org/), Zookeeper gots its name because _coordinating distributed systems is a zoo_.

At its core, Zookeeper is simple to understand. Think of it as a hierarchical filesystem or a tree. The basic building block of Zookeeper is a **znode**. A znode can store data (like a file) or have child znodes (like a directory). The overall design of Zookeeper provides for a highly available system consisting of znodes that make up a hierarchical namespace. The following is a representation of znodes:

Zookeeper can be run as a single server in standalone mode or on a cluster of machines in replicated mode, called an _ensemble_. High availability in replicated mode is achieved by ensuring modifications to the znodes tree are replicated to a majority of the ensemble. If a minority of machines in the ensemble fail, at least one live machine in the ensemble will have the latest state. Let’s consider an example. Suppose we have five machines (A, B, C, D and E) running a Zookeeper ensemble. A majority of the machines, called quorum, need an update. Machines A, C, and E get the update. Now, if a minority of the machines fail, two in this case, the service should continue to function correctly. Let’s say machines A and E fail. Then there’s at least one machine, C which has the latest state. The other two surviving machines, B and D, can catch up to the latest state. This seemingly simple algorithm to maintain high availability is notoriously hard to program correctly.

Zookeeper uses a protocol called **Zab** to implement this. The finer details of the protocol are outside the scope of this text. However, at a high level the protocol operates in two phases. The first phase involves selecting a leader. Others become followers and synchronize their state with the leader. In the second phase, all write requests are forwarded to the leader, which broadcasts the update to followers. Once, the change is committed to the majority of the machines, the client requesting the change is apprised of a successful commit. All machines in the ensemble write updates to the disk before updating their in-memory copies of the znode tree. Client can direct read requests to any machine. The requests are fulfilled from the memory resulting in fast query responses.

If the leader fails, a new election is held. If the former leader comes back up, it joins as a follower. Zab is similar to another well-known leader election protocol called **Paxos**, but differs in several respects.

## Data model[](https://www.educative.io/courses/introduction-to-big-data-and-hadoop/zookeeper-intro#Data-model)

Znodes have an associated access control list (ACL) that governs who can read or write a particular znode. Furthermore, the reads of and writes to a znode are atomic. A read/write request completes successfully or fails. It never reads or writes partial data. Znodes are referenced by paths. “/config/hostname” refers to a child znode “hostname”, which is nested under a parent znode “config”. Referencing znodes using paths allows us to visualize Zookeeper as a hierarchical filesystem.

Apart from ephemeral znodes, there are _sequential znodes_ which have a sequence number attached as part of the name. The sequence number is the value of a monotonically increasing counter, maintained by the parent znode. The sequence numbers can impose a global ordering on events in a distributed system. We’ll later see them in action when implementing a distributed lock. Zookeeper can set watches on znodes that trigger when a znode is changed. A possible watches use case is monitoring changing configurations for an application. An application can set a watch on a znode containing some configuration and be notified when it is changed by a different application.


#### Journal Node
- In order for the Standby node to keep its state synchronized with the Active node, both nodes communicate with a group of separate daemons called “JournalNodes” (JNs).
- The Daemons are the processes that run in the background of the system. The components of Hadoop known as daemons include NameNode, Secondary NameNode, DataNode, JobTracker, and TaskTracker. Each daemon conducts its operations autonomously within its JVM. Hadoop Daemons are a group of Hadoop processes that work together. 
